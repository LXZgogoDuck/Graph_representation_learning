{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwLX9xkQgWo8"
   },
   "source": [
    "# Practical 2\n",
    "## A Graph Neural Network Model for Node Classification\n",
    "### (and how to build it from scratch)\n",
    "\n",
    "### Overview\n",
    "In this practical, you will implement the base graph neural network model from scratch (without using PyTorch Geometric), and prepare it for node classification. You will train your model for node classification on Cora dataset, which is an academic citation network. The task is to predict the category of each paper (which corresponds to a node in the citation network) among the seven available. We introduced the base graph neural network model in the course, as an instance of message passing neural networks, and studied a particular parameterization of this model.\n",
    "\n",
    "For each part and task, answer the given questions and complete the code where missing (this is indicated by the `pass` keyword)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIKfciO1h74B"
   },
   "source": [
    "## Part 0: Installing dependencies\n",
    "Despite the fact we will not use PyTorch Geometric to build the GNN, we will use it to handle the Cora dataset, as it is included in the base distribution.\n",
    "Follow the given instructions to set up your Colab notebook correctly.\n",
    "\n",
    "First of all, we advice you to enable GPU acceleration for your notebook. This can be done by navigating to `Runtime > Change runtime type > Hardware accelerator (GPU) > Save`. You may getting an error explaining that no GPUs are currently available. This is fine, you don't really need them for this practical, however they'll make your computations significantly faster.\n",
    "\n",
    "Some other tips & tricks:\n",
    "- press `Shift + Enter` to run a cell and move to the next one (`Ctrl + Enter` to only run it)\n",
    "- when you execute a cell, the variables you create are saved into a global namespace. As a consequence, changes in the code will not take effect until you re-run that specific cell.\n",
    "- remember to save your notebook every once in a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5828,
     "status": "ok",
     "timestamp": 1698883347330,
     "user": {
      "displayName": "Emily Jin",
      "userId": "00213674014052790913"
     },
     "user_tz": 0
    },
    "id": "rb2wIZQdTsDG",
    "outputId": "c29a7d84-9e33-461a-c652-83e15ee5b323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\r\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch version installed on this system\n",
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 22431,
     "status": "ok",
     "timestamp": 1698883371501,
     "user": {
      "displayName": "Emily Jin",
      "userId": "00213674014052790913"
     },
     "user_tz": 0
    },
    "id": "uvLG28XMUSCF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download the corresponding PyTorch Geometric module\n",
    "\"\"\"\n",
    "Assign to TORCH with what you get from the cell above. E.g., export TORCH=1.12.1+cu113\n",
    "# \"\"\"\n",
    "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.0.html\n",
    "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.4.0.html\n",
    "# !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwIPZRdzlSTY"
   },
   "source": [
    "## Part 1: Building a GNN\n",
    "In this part, you will build a GNN using PyTorch, aiming to emulate the PyTorch Geometric modules. You will construct your neural network using PyTorch nn modules, and build training and testing helper functions to train and evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh2SbPurluzN"
   },
   "source": [
    "### Task 1.1\n",
    "Understand the base structure of a PyTorch neural network model, namely `torch.nn.Module`, and build your GNN as a class with its own instantiation of `__init__` and `forward`. You will define GNN components within `__init__`, and construct the GNN data flow in `forward`. We recommend that you construct your model such that the number of layers is parametrized. That is, you can increase or decrease the number of layers simply by changing the range of an inner loop that creates the needed layers.\n",
    "\n",
    "Furthermore, as a node classification GNN is composed by a sequence of Graph Convolution Layers and by a Multi Layer Perceptron head used to classify each node, we suggest to define a separate module for each of these components, and then aggregate them together in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "muJ-TbV6muhI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairyliu/anaconda3/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Let's first import all the things we are gonna need for this task\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "# torch_geometric only used to load the Cora dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as U\n",
    "    \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\")\n",
    "num_nodes = dataset.data.num_nodes\n",
    "num_edges = dataset.data.num_edges // 2\n",
    "num_features = dataset.num_node_features\n",
    "num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n",
      "5278\n",
      "1433\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(num_nodes)\n",
    "print(num_edges)\n",
    "print(num_features)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rwDJj57m-H3"
   },
   "source": [
    "Please complete the following classes, adding the respective `__init__` and `forward` methods.  \n",
    "Please construct your model such that the number of layers is parametrized. That is, you can increase or decrease the number of layers simply by changing the range of an inner loop that creates the needed layers. In general, try to hard-code the least amount of behaviour inside your network. Prefer, instead, the use of parameters passed during initialization, as it will come in handy later on (e.g., you can chosse to hard code the activation functions used for each model but also leave them as a model parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0jVmuI-u0uF"
   },
   "source": [
    "A `GNNLayer` computes the operations seen in the lectures, that is:\n",
    "\n",
    "$ h_i^{(t+1)} = \\sigma(W^{(t)}_{self} h_i^{(t)} + W^{(t)}_{neigh} \\sum_{j \\in neigh(i)} h_j^{(t)})$.\n",
    "\n",
    "Remember that PyTorch is realy efficient in parallelizing computations by vectorizing them. In other words, you don't need to (you must not!) loop over all the neighbours of each node to compute the summation in the above equation. Use instead matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TU3I3uOMNlIe"
   },
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "\n",
    "    # create one single layer\n",
    "    # H Wself + A H Wneigh.  H: n*input, Wself: input*output\n",
    "    # _init_ specifies parameters in one layer (the knowledge passing function is one layer's operation)\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation=nn.ReLU()):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        \n",
    "        # Define self and neighbor transformation matrices\n",
    "        self.W_self = nn.Linear(input_dim, output_dim)  # W_self matrix\n",
    "        self.W_neigh = nn.Linear(input_dim, output_dim) # W_neigh matrix\n",
    "        self.activation = activation \n",
    "    \n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        # Apply the self-node transformation\n",
    "        self_feats = self.W_self(node_feats)  # Shape: (num_nodes, output_dim)\n",
    "        \n",
    "        # Aggregate neighbor features\n",
    "        neigh_feats = torch.matmul(adj_matrix, node_feats)  # Shape: (num_nodes, input_dim)\n",
    "        neigh_feats = self.W_neigh(neigh_feats)  \n",
    "        \n",
    "        # Sum self and neighbor features, then apply activation\n",
    "        out = self_feats + neigh_feats\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOXzfLBRuFJ5"
   },
   "source": [
    "The `GNNModule` is a collection of GNNLayers. In PyTorch you can create a list of layers by using `nn.ModuleList(layer_1, layer_2, ...)` or, equivalentely `nn.ModuleList(*layer_list)`. Remember activation functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TEnA7gA9pYU3"
   },
   "outputs": [],
   "source": [
    "class GNNModule(nn.Module):\n",
    " \n",
    "    # loop through multiple layers\n",
    "    # input layer: input->hidden, (hidden->hidden), hidden->output\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, act_fn=nn.ReLU()):\n",
    "        super(GNNModule, self).__init__()\n",
    "        layer_list = []\n",
    "        layer_in = GNNLayer(input_dim, hidden_dim, act_fn)\n",
    "        layer_list.append(layer_in)\n",
    "        \n",
    "        for i in range(num_layers - 2):\n",
    "            layer = GNNLayer(hidden_dim, hidden_dim, act_fn)\n",
    "            layer_list.append(layer)\n",
    "        \n",
    "        layer_out = GNNLayer(hidden_dim, output_dim, act_fn)\n",
    "        layer_list.append(layer_out)\n",
    "        \n",
    "        self.layers = nn.ModuleList(layer_list)\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, adj_matrix)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbm-Dwnfs8kZ"
   },
   "source": [
    "The `MLPModule` is a classification head that you apply to each node in the input graph after applying the gnn layers. It is a collection of `nn.Linear` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kRzeNjIWXLpD"
   },
   "outputs": [],
   "source": [
    "class MLPModule(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, act_fn=nn.ReLU()):\n",
    "        super(MLPModule, self).__init__()\n",
    "        \n",
    "        layer_list = []\n",
    "        layer_list.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layer_list.append(act_fn)\n",
    "        \n",
    "        for i in range(num_layers - 2):\n",
    "            layer_list.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layer_list.append(act_fn)\n",
    "        \n",
    "        # Output layer (without activation)\n",
    "        layer_list.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        out = self.layers(x)\n",
    " \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3sUDyK6wUMc"
   },
   "source": [
    "Let's aggregate everything in a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cNVsovJTYk7X"
   },
   "outputs": [],
   "source": [
    "class CoraNodeClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self, gnn_input_dim, gnn_hidden_dim, gnn_output_dim, mlp_hidden_dim, mlp_output_dim, gnn_num_layers=2, mlp_num_layers=2, gnn_act_fn = nn.ReLU(), mlp_act_fn = nn.ReLU()):\n",
    "        super(CoraNodeClassification, self).__init__()\n",
    "        self.gnnModule = GNNModule(gnn_input_dim, gnn_hidden_dim, gnn_output_dim, gnn_num_layers, gnn_act_fn)\n",
    "        self.mlpModule = MLPModule(gnn_output_dim, mlp_hidden_dim, mlp_output_dim, mlp_num_layers, mlp_act_fn)\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        y = self.gnnModule(x, adj_matrix)\n",
    "        out = self.mlpModule(y)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    # Used to reset the weights of the network when training multiple times with\n",
    "    # different hyperparameters\n",
    "    def reset_parameters(self):\n",
    "        def _reset_module_parameters(module):\n",
    "            for layer in module.children():\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "                elif hasattr(layer, 'children'):\n",
    "                    for child_layer in layer.children():\n",
    "                        _reset_module_parameters(child_layer)\n",
    "\n",
    "        _reset_module_parameters(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfeXY4_koR86"
   },
   "source": [
    "### Task 1.2\n",
    "Create dedicated functions for training and testing. For training, your functions should return a vector containing the train loss and accuracy after each epoch (since Cora contains a single graph one epoch corresponds to one training iteration). You can also print out those values while the training it's running. However, given the large number of epochs, we advise you print every 8 steps or so. Your test function should return the final accuracy on the test set. If everything works correctly you should get around ~75% accuracy.\n",
    "\n",
    "Cora uses a masking approach to divide between the train and test set. Therefore, to compute the loss/accuracy, mask out the part of your output based on the current settings, e.g., `loss = model.loss_fn(y[data.train_mask], data.y[data.train_mask])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UA98VowGrOtM"
   },
   "outputs": [],
   "source": [
    "# The Cora dataset contains a single graph of 2708 nodes (i.e., papers).\n",
    "# We will use some of the nodes as training set and some as test set.\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# According to the GNN equations, we need the adjacency matrix to compute each\n",
    "# layer convolution. The following line convert the sparse data\n",
    "# (i.e., list of edges) stored in the dataset in a single dense matrix.\n",
    "adj_matrix = U.to_dense_adj(data.edge_index).squeeze(0)\n",
    "\n",
    "# Define the hyperparameters we are gonna use:\n",
    "params = {\n",
    "    \"hidden_features\": 128,\n",
    "    \"gnn_output_dim\": 64,\n",
    "    \"mlp_hidden_dim\": 32,\n",
    "    \"num_gcn_layers\": 2,\n",
    "    \"num_mlp_layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 300,\n",
    "}\n",
    "\n",
    "# Remember to move the model to the correct device using `.to(device)`\n",
    "# The arguments to be passed to the class CoraNodeClassification __init__ functions\n",
    "# depends on how you defined the method. This is an example.\n",
    "model = CoraNodeClassification(num_features,\n",
    "                               params[\"hidden_features\"],\n",
    "                               params[\"gnn_output_dim\"],\n",
    "                               params[\"mlp_hidden_dim\"],\n",
    "                               num_classes,\n",
    "                               params[\"num_gcn_layers\"],\n",
    "                               params[\"num_mlp_layers\"],\n",
    "                               gnn_act_fn=nn.ReLU(),  \n",
    "                               mlp_act_fn=nn.ReLU()\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9TmqadEefi5X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(model, data, adj_matrix, params):\n",
    "    model.eval() \n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, adj_matrix)\n",
    "\n",
    "    _, pred = out[data.test_mask].max(dim=1)\n",
    "    test_accuracy = 100* (pred == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "# By testing a random initialized module you should get around ~15% accuracy\n",
    "test(model, data, adj_matrix, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ycmyiU4VYOqU"
   },
   "outputs": [],
   "source": [
    "# def train(model, data, adj_matrix, params):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "#     num_epochs = params[\"num_epochs\"]\n",
    "\n",
    "#     # Lists to store loss and accuracy for each epoch\n",
    "#     losses = []\n",
    "#     accuracies = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "\n",
    "#         # Forward pass\n",
    "#         out = model(data.x, adj_matrix)\n",
    "\n",
    "#         # Calculate loss (using only the training mask)\n",
    "#         train_loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         train_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Calculate accuracy (using only the training mask)\n",
    "#         _, pred = out[data.train_mask].max(dim=1)\n",
    "#         train_acc = (pred == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n",
    "\n",
    "#         losses.append(train_loss.item())\n",
    "#         accuracies.append(train_acc)\n",
    "\n",
    "#         # Print every 8 steps\n",
    "#         if (epoch + 1) % 8 == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss.item():.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "#     return losses, accuracies\n",
    "\n",
    "# train_losses, train_accuracies = train(model, data, adj_matrix, params)\n",
    "# # Now you shold get around ~75-77%!\n",
    "# test_accuracy = test(model, data, adj_matrix, params)\n",
    "# print(\"Final test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, adj_matrix, params):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        out = model(data.x, adj_matrix)\n",
    "        train_loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, train_pred = out[data.train_mask].max(dim=1)\n",
    "        train_acc = (train_pred == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n",
    "\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            test_out = model(data.x, adj_matrix)\n",
    "            _, test_pred = test_out[data.test_mask].max(dim=1)\n",
    "            test_acc = (test_pred == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)   \n",
    "\n",
    "        # Print every 8 steps\n",
    "        if (epoch + 1) % 8 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf6ZviVq3CVJ"
   },
   "source": [
    "#### Task 1.2.1\n",
    "Can you modify the training function to return the test accuracy at every step as well?\n",
    "By plotting the train accuracy and test accuracy over time, what can you observe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can observe that: with the number of epoches increasing, the training and testing accuracy increase. (While training accracy increases faster than testing accuracy, with its gradient larger) But at some point, the both accuracies reach the platform, which cannot be improved over time. And testing accuracy reach the peak faster than training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vGI6Fo7n3VBz",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/300, Loss: 1.8243, Train Accuracy: 0.2929, Test Accuracy: 0.1880\n",
      "Epoch 16/300, Loss: 1.7201, Train Accuracy: 0.3714, Test Accuracy: 0.2610\n",
      "Epoch 24/300, Loss: 1.6141, Train Accuracy: 0.5286, Test Accuracy: 0.3300\n",
      "Epoch 32/300, Loss: 1.5103, Train Accuracy: 0.6857, Test Accuracy: 0.4080\n",
      "Epoch 40/300, Loss: 1.4067, Train Accuracy: 0.7357, Test Accuracy: 0.4690\n",
      "Epoch 48/300, Loss: 1.2990, Train Accuracy: 0.8071, Test Accuracy: 0.5420\n",
      "Epoch 56/300, Loss: 1.1874, Train Accuracy: 0.8571, Test Accuracy: 0.5900\n",
      "Epoch 64/300, Loss: 1.0754, Train Accuracy: 0.8857, Test Accuracy: 0.6260\n",
      "Epoch 72/300, Loss: 0.9663, Train Accuracy: 0.9357, Test Accuracy: 0.6620\n",
      "Epoch 80/300, Loss: 0.8641, Train Accuracy: 0.9429, Test Accuracy: 0.6930\n",
      "Epoch 88/300, Loss: 0.7695, Train Accuracy: 0.9571, Test Accuracy: 0.7050\n",
      "Epoch 96/300, Loss: 0.6815, Train Accuracy: 0.9786, Test Accuracy: 0.7100\n",
      "Epoch 104/300, Loss: 0.6027, Train Accuracy: 0.9857, Test Accuracy: 0.7120\n",
      "Epoch 112/300, Loss: 0.5318, Train Accuracy: 0.9857, Test Accuracy: 0.7180\n",
      "Epoch 120/300, Loss: 0.4679, Train Accuracy: 1.0000, Test Accuracy: 0.7230\n",
      "Epoch 128/300, Loss: 0.4107, Train Accuracy: 1.0000, Test Accuracy: 0.7250\n",
      "Epoch 136/300, Loss: 0.3600, Train Accuracy: 1.0000, Test Accuracy: 0.7240\n",
      "Epoch 144/300, Loss: 0.3157, Train Accuracy: 1.0000, Test Accuracy: 0.7250\n",
      "Epoch 152/300, Loss: 0.2774, Train Accuracy: 1.0000, Test Accuracy: 0.7300\n",
      "Epoch 160/300, Loss: 0.2441, Train Accuracy: 1.0000, Test Accuracy: 0.7310\n",
      "Epoch 168/300, Loss: 0.2153, Train Accuracy: 1.0000, Test Accuracy: 0.7330\n",
      "Epoch 176/300, Loss: 0.1905, Train Accuracy: 1.0000, Test Accuracy: 0.7330\n",
      "Epoch 184/300, Loss: 0.1689, Train Accuracy: 1.0000, Test Accuracy: 0.7340\n",
      "Epoch 192/300, Loss: 0.1502, Train Accuracy: 1.0000, Test Accuracy: 0.7360\n",
      "Epoch 200/300, Loss: 0.1338, Train Accuracy: 1.0000, Test Accuracy: 0.7390\n",
      "Epoch 208/300, Loss: 0.1194, Train Accuracy: 1.0000, Test Accuracy: 0.7430\n",
      "Epoch 216/300, Loss: 0.1068, Train Accuracy: 1.0000, Test Accuracy: 0.7410\n",
      "Epoch 224/300, Loss: 0.0957, Train Accuracy: 1.0000, Test Accuracy: 0.7420\n",
      "Epoch 232/300, Loss: 0.0859, Train Accuracy: 1.0000, Test Accuracy: 0.7400\n",
      "Epoch 240/300, Loss: 0.0772, Train Accuracy: 1.0000, Test Accuracy: 0.7440\n",
      "Epoch 248/300, Loss: 0.0696, Train Accuracy: 1.0000, Test Accuracy: 0.7430\n",
      "Epoch 256/300, Loss: 0.0628, Train Accuracy: 1.0000, Test Accuracy: 0.7420\n",
      "Epoch 264/300, Loss: 0.0568, Train Accuracy: 1.0000, Test Accuracy: 0.7430\n",
      "Epoch 272/300, Loss: 0.0514, Train Accuracy: 1.0000, Test Accuracy: 0.7420\n",
      "Epoch 280/300, Loss: 0.0467, Train Accuracy: 1.0000, Test Accuracy: 0.7410\n",
      "Epoch 288/300, Loss: 0.0425, Train Accuracy: 1.0000, Test Accuracy: 0.7400\n",
      "Epoch 296/300, Loss: 0.0387, Train Accuracy: 1.0000, Test Accuracy: 0.7420\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies, test_accuracies= train(model, data, adj_matrix, params)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_accuracies)\n",
    "plt.plot(test_accuracies)\n",
    "plt.show()\n",
    "\n",
    "print(\"Final test accuracy:\", test_accuracies[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imWx-ZW-9Ydo"
   },
   "source": [
    "## Part 2: Ablation Studies (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R9lWeiNsLpD"
   },
   "source": [
    "### 2.1: Importance of GNN and MLP modules\n",
    "Add a parameter to the `CoraNodeClassification` class that allows you to select which modules to enable: only GNN, only MLP, or both. For example, your `forward` should look similarly to the following:\n",
    "\n",
    "```python\n",
    "def forward(self, x, adj_matrix):\n",
    "    if self.gnn is not None:\n",
    "      x = self.gnn(x, adj_matrix)\n",
    "    \n",
    "    if self.mlp is not None:\n",
    "      x = self.mlp(x)\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "What is the impact of the two modules on the final result? Play a little around `hidden_features` and `num_layers` and briefly describe what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhFLlmGet7r_"
   },
   "source": [
    "observe: \n",
    "\n",
    "    params = {\n",
    "    \"hidden_features\": 128,\n",
    "    \"gnn_output_dim\": 7,\n",
    "    \"mlp_hidden_dim\": 7,\n",
    "    \"num_gcn_layers\": 5,\n",
    "    \"num_mlp_layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 2000, }\n",
    "    \n",
    " final test accuracy: 0.637, accuracy first increase, but at some point it reaches platform, and after a while accuracy increase and reaches to another platform (and stay around 0.63 permanently)\n",
    " \n",
    " \n",
    "observe:\n",
    "\n",
    "    params = {\n",
    "    \"hidden_features\": 128,\n",
    "    \"gnn_output_dim\": 7,\n",
    "    \"mlp_hidden_dim\": 7,\n",
    "    \"num_gcn_layers\": 3,\n",
    "    \"num_mlp_layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 2000,}\n",
    "    \n",
    " final test accuracy: 0.633, test accruacy first increases, then at around 250 epoches reaches its platform.\n",
    " But training accuracy have \"steps\".  (Why?)\n",
    " \n",
    " \n",
    " \n",
    " when setting num_gnn_layers = 2, num_mlp_layers = 10 -> test accuracies 0.38\n",
    " adding mlp-layers, make generalization power/accuracies drop greatly.\n",
    " \n",
    " when setting num_gnn_layers = 10, num_mlp_layers = 2 -> test accuracy: 0.143\n",
    " results even worse than previously.\n",
    " \n",
    " \n",
    " \n",
    " changing the output dimension of gnn layers -> better result\n",
    " \n",
    " \n",
    "     params = {\n",
    "    \"hidden_features\": 128,\n",
    "    \"gnn_output_dim\": 64,\n",
    "    \"mlp_hidden_dim\": 32,\n",
    "    \"num_gcn_layers\": 5,\n",
    "    \"num_mlp_layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 500,}\n",
    "    \n",
    "Train Accuracy: 0.9714, Test Accuracy: 0.6770\n",
    "\n",
    "    params = {\n",
    "    \"hidden_features\": 128,\n",
    "    \"gnn_output_dim\": 64,\n",
    "    \"mlp_hidden_dim\": 32,\n",
    "    \"num_gcn_layers\": 3,\n",
    "    \"num_mlp_layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 500,}\n",
    "\n",
    "Epoch 496/500, Loss: 0.0034, Train Accuracy: 1.0000, Test Accuracy: 0.7300\n",
    " \n",
    " \n",
    "    params = {\n",
    "    \"hidden_features\": 128,\n",
    "    \"gnn_output_dim\": 64,\n",
    "    \"mlp_hidden_dim\": 32,\n",
    "    \"num_gcn_layers\": 2,\n",
    "    \"num_mlp_layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 500,}\n",
    "    \n",
    " Epoch 496/500, Loss: 0.0088, Train Accuracy: 1.0000, Test Accuracy: 0.7310\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe that without GNN module or MLP module, the training and testing accuracies drop significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VW-V8RYft7GC"
   },
   "source": [
    "### 2.2: Importance of non-linearities\n",
    "Try to replace all your activation functions with identities (or simply remove them). Do the results change? What does this tell you about this specific problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4ch8LoWCkWr"
   },
   "source": [
    "### After replacing all acitivation functions with identities, the results indeed change:\n",
    "    Epoch 296/300, Loss: 0.0198, Train Accuracy: 1.0000, Test Accuracy: 0.7290\n",
    "    With same parameters, the test accuracy drop slightly. So we can conclude that the non-linearities (relu/sig) is important in training\n",
    "    \n",
    "    (doesn't change much? Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
